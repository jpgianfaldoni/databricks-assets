{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLOps Best Practices Guide: Post-Deployment Workflows\n",
        "\n",
        "## üìö Overview\n",
        "\n",
        "This notebook provides a comprehensive guide to MLOps best practices after your initial model has been trained and deployed. It covers:\n",
        "\n",
        "1. **Creating Challenger Models**: How to iteratively improve your model\n",
        "2. **Version Management**: When to create new versions vs. new models\n",
        "3. **A/B Testing**: Comparing model performance in production\n",
        "4. **Continuous Improvement**: Monitoring, retraining, and automated workflows\n",
        "5. **Model Governance**: Promotion strategies and approval workflows\n",
        "6. **Production Best Practices**: Scaling, monitoring, and maintenance\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- An existing model registered in Unity Catalog (we'll use the wine_classifier_model from the previous notebook)\n",
        "- MLflow 3.x\n",
        "- Access to Unity Catalog\n",
        "- Databricks Runtime ML 13.0 or higher\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initial Setup\n",
        "import mlflow\n",
        "from mlflow import MlflowClient\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up Unity Catalog\n",
        "mlflow.set_registry_uri(\"databricks-uc\")\n",
        "\n",
        "# Initialize MLflow client\n",
        "client = MlflowClient()\n",
        "\n",
        "# Configuration\n",
        "CATALOG = \"jpg_ws_us_3\"\n",
        "SCHEMA = \"default\"\n",
        "MODEL_NAME = \"wine_classifier_model\"\n",
        "FULL_MODEL_NAME = f\"{CATALOG}.{SCHEMA}.{MODEL_NAME}\"\n",
        "\n",
        "# Get current user\n",
        "current_user = spark.sql(\"SELECT current_user() as user\").collect()[0]['user']\n",
        "\n",
        "print(f\"MLflow Version: {mlflow.__version__}\")\n",
        "print(f\"Current User: {current_user}\")\n",
        "print(f\"Working with Model: {FULL_MODEL_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Understanding Model Versioning Strategy\n",
        "\n",
        "### üîë Key Concepts\n",
        "\n",
        "**When to create a NEW VERSION of the SAME model:**\n",
        "- Same algorithm/approach with different hyperparameters\n",
        "- Retrained on updated data (same features)\n",
        "- Minor code optimizations\n",
        "- Bug fixes that don't change the model architecture\n",
        "\n",
        "**When to create a NEW MODEL:**\n",
        "- Different algorithm (e.g., switching from Random Forest to XGBoost)\n",
        "- Significant feature engineering changes\n",
        "- Different problem formulation\n",
        "- Major architectural changes\n",
        "\n",
        "### Model Naming Convention\n",
        "\n",
        "```\n",
        "catalog.schema.model_name\n",
        "\n",
        "Examples:\n",
        "- jpg_ws_us_3.default.wine_classifier_rf    # Random Forest\n",
        "- jpg_ws_us_3.default.wine_classifier_xgb   # XGBoost\n",
        "- jpg_ws_us_3.default.wine_classifier_ensemble  # Ensemble\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Current Model Status\n",
        "def get_model_status(model_name):\n",
        "    \"\"\"Get comprehensive status of a model and its versions\"\"\"\n",
        "    try:\n",
        "        # Get model info\n",
        "        model = client.get_registered_model(model_name)\n",
        "        \n",
        "        # Get all versions\n",
        "        versions = client.search_model_versions(f\"name='{model_name}'\")\n",
        "        \n",
        "        print(f\"Model: {model_name}\")\n",
        "        print(f\"Description: {model.description if model.description else 'No description'}\")\n",
        "        print(f\"Total Versions: {len(versions)}\")\n",
        "        \n",
        "        # Check for aliases\n",
        "        if hasattr(model, 'aliases'):\n",
        "            print(\"\\nCurrent Aliases:\")\n",
        "            for alias, version in model.aliases.items():\n",
        "                print(f\"  {alias}: Version {version}\")\n",
        "        \n",
        "        # Show recent versions\n",
        "        print(\"\\nRecent Versions (last 3):\")\n",
        "        for v in sorted(versions, key=lambda x: x.version, reverse=True)[:3]:\n",
        "            print(f\"  Version {v.version}:\")\n",
        "            print(f\"    Created: {datetime.fromtimestamp(v.creation_timestamp/1000).strftime('%Y-%m-%d %H:%M')}\")\n",
        "            print(f\"    Run ID: {v.run_id}\")\n",
        "            \n",
        "            # Get metrics from the run\n",
        "            run = client.get_run(v.run_id)\n",
        "            if run.data.metrics:\n",
        "                key_metrics = ['test_accuracy', 'test_f1', 'val_accuracy']\n",
        "                metrics = {k: v for k, v in run.data.metrics.items() if k in key_metrics}\n",
        "                if metrics:\n",
        "                    print(f\"    Metrics: {metrics}\")\n",
        "        \n",
        "        return model, versions\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing model: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "# Check current champion model\n",
        "model, versions = get_model_status(FULL_MODEL_NAME)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Creating a Challenger Model\n",
        "\n",
        "### üéØ Challenger Model Strategy\n",
        "\n",
        "A **challenger model** is a new version that competes with the current champion. The workflow is:\n",
        "\n",
        "1. **Champion** ‚Üí Current production model\n",
        "2. **Challenger** ‚Üí New model being tested\n",
        "3. **A/B Testing** ‚Üí Compare performance in production\n",
        "4. **Promotion** ‚Üí Challenger becomes champion if better\n",
        "\n",
        "### Best Practice Workflow:\n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "    A[Train New Model] --> B[Register as New Version]\n",
        "    B --> C[Tag as Challenger]\n",
        "    C --> D[A/B Test in Production]\n",
        "    D --> E{Better than Champion?}\n",
        "    E -->|Yes| F[Promote to Champion]\n",
        "    E -->|No| G[Archive or Iterate]\n",
        "    F --> H[Previous Champion ‚Üí Archived]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Creating a Challenger Model (Improved Version)\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import mlflow.sklearn\n",
        "\n",
        "# Load data (same as before)\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# IMPORTANT: Set experiment for challenger model\n",
        "experiment_name = f\"/Users/{current_user}/wine_classifier_challenger_experiments\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "print(f\"Creating challenger model experiment: {experiment_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Challenger Model with Improved Hyperparameters\n",
        "with mlflow.start_run(run_name=\"Challenger_v2_Enhanced_RF\") as run:\n",
        "    \n",
        "    # Log experiment metadata\n",
        "    mlflow.set_tag(\"model_type\", \"RandomForestClassifier\")\n",
        "    mlflow.set_tag(\"purpose\", \"challenger\")\n",
        "    mlflow.set_tag(\"improvement_strategy\", \"enhanced_hyperparameters\")\n",
        "    mlflow.set_tag(\"challenger_version\", \"v2\")\n",
        "    \n",
        "    # Log what improvements we're trying\n",
        "    improvement_notes = \"\"\"\n",
        "    Improvements over champion:\n",
        "    1. Expanded hyperparameter search space\n",
        "    2. Added class weight balancing\n",
        "    3. Increased number of estimators\n",
        "    4. Added bootstrap parameter tuning\n",
        "    \"\"\"\n",
        "    mlflow.log_param(\"improvements\", improvement_notes)\n",
        "    \n",
        "    # Enhanced hyperparameter grid\n",
        "    enhanced_param_grid = {\n",
        "        'n_estimators': [100, 200, 300],  # More trees\n",
        "        'max_depth': [10, 20, 30, None],   # More depth options\n",
        "        'min_samples_split': [2, 5, 10],    \n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2'],   # Feature sampling\n",
        "        'bootstrap': [True, False],         # Bootstrap sampling\n",
        "        'class_weight': ['balanced', None]  # Handle imbalanced classes\n",
        "    }\n",
        "    \n",
        "    # Train model with enhanced parameters\n",
        "    rf_challenger = RandomForestClassifier(random_state=42)\n",
        "    \n",
        "    # Grid search with more extensive CV\n",
        "    grid_search = GridSearchCV(\n",
        "        rf_challenger,\n",
        "        enhanced_param_grid,\n",
        "        cv=10,  # More folds for better validation\n",
        "        scoring='f1_weighted',  # Focus on F1 score\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    print(\"Training challenger model with enhanced hyperparameters...\")\n",
        "    grid_search.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Get best model\n",
        "    challenger_model = grid_search.best_estimator_\n",
        "    \n",
        "    # Evaluate\n",
        "    test_predictions = challenger_model.predict(X_test_scaled)\n",
        "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "    test_f1 = f1_score(y_test, test_predictions, average='weighted')\n",
        "    \n",
        "    # Log metrics\n",
        "    mlflow.log_params(grid_search.best_params_)\n",
        "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
        "    mlflow.log_metric(\"test_f1\", test_f1)\n",
        "    mlflow.log_metric(\"cv_best_score\", grid_search.best_score_)\n",
        "    \n",
        "    # Create pipeline for deployment\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    challenger_pipeline = Pipeline([\n",
        "        ('scaler', scaler),\n",
        "        ('classifier', challenger_model)\n",
        "    ])\n",
        "    \n",
        "    # Log model\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=challenger_pipeline,\n",
        "        artifact_path=\"model\",\n",
        "        signature=mlflow.models.infer_signature(X_test, test_predictions)\n",
        "    )\n",
        "    \n",
        "    challenger_run_id = run.info.run_id\n",
        "    \n",
        "    print(f\"\\nChallenger Model Results:\")\n",
        "    print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"  Test F1 Score: {test_f1:.4f}\")\n",
        "    print(f\"  Best CV Score: {grid_search.best_score_:.4f}\")\n",
        "    print(f\"  Run ID: {challenger_run_id}\")\n",
        "    print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Registering Challenger Model - Best Practices\n",
        "\n",
        "### ‚ö° IMPORTANT: Same Model, New Version!\n",
        "\n",
        "For iterative improvements of the same algorithm, register as a **new version** of the existing model, NOT a new model.\n",
        "\n",
        "### Decision Tree:\n",
        "```\n",
        "Is it the same algorithm? ‚Üí YES ‚Üí New Version of Same Model\n",
        "                         ‚Üì\n",
        "                         NO ‚Üí Create New Model\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register Challenger as New Version of SAME Model\n",
        "try:\n",
        "    # Register as new version of the existing model\n",
        "    challenger_version = mlflow.register_model(\n",
        "        model_uri=f\"runs:/{challenger_run_id}/model\",\n",
        "        name=FULL_MODEL_NAME  # SAME model name - creates new version\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úì Challenger registered as Version {challenger_version.version} of {FULL_MODEL_NAME}\")\n",
        "    \n",
        "    # Add comprehensive metadata\n",
        "    client.update_model_version(\n",
        "        name=FULL_MODEL_NAME,\n",
        "        version=challenger_version.version,\n",
        "        description=f\"\"\"\n",
        "        Challenger model created on {datetime.now().strftime('%Y-%m-%d')}\n",
        "        \n",
        "        Improvements:\n",
        "        - Enhanced hyperparameter search space\n",
        "        - Added class weight balancing\n",
        "        - Increased estimators and CV folds\n",
        "        \n",
        "        Performance:\n",
        "        - Test Accuracy: {test_accuracy:.4f}\n",
        "        - Test F1 Score: {test_f1:.4f}\n",
        "        \n",
        "        Status: Ready for A/B testing against champion\n",
        "        \"\"\"\n",
        "    )\n",
        "    \n",
        "    # Add version tags\n",
        "    tags_to_add = {\n",
        "        \"model_type\": \"RandomForestClassifier\",\n",
        "        \"purpose\": \"challenger\",\n",
        "        \"test_accuracy\": str(test_accuracy),\n",
        "        \"test_f1\": str(test_f1),\n",
        "        \"training_date\": datetime.now().strftime('%Y-%m-%d'),\n",
        "        \"created_by\": current_user\n",
        "    }\n",
        "    \n",
        "    for key, value in tags_to_add.items():\n",
        "        client.set_model_version_tag(\n",
        "            name=FULL_MODEL_NAME,\n",
        "            version=challenger_version.version,\n",
        "            key=key,\n",
        "            value=value\n",
        "        )\n",
        "    \n",
        "    # IMPORTANT: Set challenger alias\n",
        "    client.set_registered_model_alias(\n",
        "        name=FULL_MODEL_NAME,\n",
        "        alias=\"challenger\",\n",
        "        version=challenger_version.version\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úì Version {challenger_version.version} tagged as 'challenger'\")\n",
        "    print(f\"‚úì Ready for A/B testing\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error registering challenger: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Comparing Champion vs Challenger Models\n",
        "\n",
        "### üìä Model Comparison Framework\n",
        "\n",
        "Before promoting a challenger to champion, you need comprehensive comparison:\n",
        "\n",
        "1. **Statistical Performance**: Accuracy, Precision, Recall, F1\n",
        "2. **Business Metrics**: Revenue impact, user satisfaction\n",
        "3. **Operational Metrics**: Latency, throughput, resource usage\n",
        "4. **Robustness**: Performance on edge cases, data drift\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Champion vs Challenger Models\n",
        "def compare_models(model_name, champion_alias=\"champion\", challenger_alias=\"challenger\"):\n",
        "    \"\"\"Compare performance of champion and challenger models\"\"\"\n",
        "    \n",
        "    try:\n",
        "        # Load both models\n",
        "        print(\"Loading models...\")\n",
        "        champion_model = mlflow.pyfunc.load_model(f\"models:/{model_name}@{champion_alias}\")\n",
        "        challenger_model = mlflow.pyfunc.load_model(f\"models:/{model_name}@{challenger_alias}\")\n",
        "        \n",
        "        # Get model versions\n",
        "        model_info = client.get_registered_model(model_name)\n",
        "        champion_version = model_info.aliases.get(champion_alias)\n",
        "        challenger_version = model_info.aliases.get(challenger_alias)\n",
        "        \n",
        "        print(f\"Champion: Version {champion_version}\")\n",
        "        print(f\"Challenger: Version {challenger_version}\")\n",
        "        \n",
        "        # Prepare test data\n",
        "        test_data = pd.DataFrame(X_test, columns=wine.feature_names)\n",
        "        \n",
        "        # Get predictions\n",
        "        import time\n",
        "        \n",
        "        # Champion predictions and timing\n",
        "        start_time = time.time()\n",
        "        champion_predictions = champion_model.predict(test_data)\n",
        "        champion_inference_time = time.time() - start_time\n",
        "        \n",
        "        # Challenger predictions and timing\n",
        "        start_time = time.time()\n",
        "        challenger_predictions = challenger_model.predict(test_data)\n",
        "        challenger_inference_time = time.time() - start_time\n",
        "        \n",
        "        # Calculate metrics\n",
        "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "        \n",
        "        metrics = {\n",
        "            'Champion': {\n",
        "                'version': champion_version,\n",
        "                'accuracy': accuracy_score(y_test, champion_predictions),\n",
        "                'precision': precision_score(y_test, champion_predictions, average='weighted'),\n",
        "                'recall': recall_score(y_test, champion_predictions, average='weighted'),\n",
        "                'f1': f1_score(y_test, champion_predictions, average='weighted'),\n",
        "                'inference_time': champion_inference_time,\n",
        "                'avg_latency_ms': (champion_inference_time / len(test_data)) * 1000\n",
        "            },\n",
        "            'Challenger': {\n",
        "                'version': challenger_version,\n",
        "                'accuracy': accuracy_score(y_test, challenger_predictions),\n",
        "                'precision': precision_score(y_test, challenger_predictions, average='weighted'),\n",
        "                'recall': recall_score(y_test, challenger_predictions, average='weighted'),\n",
        "                'f1': f1_score(y_test, challenger_predictions, average='weighted'),\n",
        "                'inference_time': challenger_inference_time,\n",
        "                'avg_latency_ms': (challenger_inference_time / len(test_data)) * 1000\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # Create comparison DataFrame\n",
        "        comparison_df = pd.DataFrame(metrics).T\n",
        "        comparison_df['improvement_%'] = ((comparison_df.loc['Challenger'] - comparison_df.loc['Champion']) / comparison_df.loc['Champion'] * 100)\n",
        "        \n",
        "        # Display results\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"MODEL COMPARISON RESULTS\")\n",
        "        print(\"=\"*60)\n",
        "        display(comparison_df)\n",
        "        \n",
        "        # Determine winner\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"RECOMMENDATION\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        if metrics['Challenger']['f1'] > metrics['Champion']['f1']:\n",
        "            improvement = (metrics['Challenger']['f1'] - metrics['Champion']['f1']) * 100\n",
        "            print(f\"‚úì Challenger shows {improvement:.2f}% improvement in F1 score\")\n",
        "            print(f\"‚úì Recommendation: PROMOTE challenger to champion\")\n",
        "        else:\n",
        "            print(f\"‚úó Challenger does not improve upon champion\")\n",
        "            print(f\"‚úó Recommendation: KEEP current champion\")\n",
        "        \n",
        "        return comparison_df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error comparing models: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Compare the models\n",
        "comparison_results = compare_models(FULL_MODEL_NAME)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. A/B Testing in Production\n",
        "\n",
        "### üîÄ Traffic Splitting Strategies\n",
        "\n",
        "A/B testing allows you to safely test your challenger model with real production traffic:\n",
        "\n",
        "1. **Canary Deployment**: Start with 5% traffic, gradually increase\n",
        "2. **50/50 Split**: Equal traffic for statistical significance\n",
        "3. **Multi-Armed Bandit**: Dynamically adjust traffic based on performance\n",
        "\n",
        "### Implementation Approaches:\n",
        "\n",
        "1. **Model Serving Endpoints**: Use Databricks Model Serving with traffic routing\n",
        "2. **Application-Level**: Implement routing logic in your application\n",
        "3. **Feature Flags**: Use feature flag services for dynamic control\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A/B Testing Simulation\n",
        "class ABTestingFramework:\n",
        "    \"\"\"Simulate A/B testing between champion and challenger models\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name, test_duration_hours=24):\n",
        "        self.model_name = model_name\n",
        "        self.test_duration_hours = test_duration_hours\n",
        "        self.results = {'champion': [], 'challenger': []}\n",
        "        \n",
        "    def route_traffic(self, traffic_split=0.5):\n",
        "        \"\"\"Route traffic based on split percentage\"\"\"\n",
        "        return 'challenger' if np.random.random() < traffic_split else 'champion'\n",
        "    \n",
        "    def simulate_production_traffic(self, n_requests=1000, challenger_traffic=0.2):\n",
        "        \"\"\"Simulate production traffic with A/B split\"\"\"\n",
        "        \n",
        "        print(f\"Simulating A/B Test: {challenger_traffic*100:.0f}% challenger, {(1-challenger_traffic)*100:.0f}% champion\")\n",
        "        print(f\"Total requests: {n_requests}\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        # Load models\n",
        "        champion = mlflow.pyfunc.load_model(f\"models:/{self.model_name}@champion\")\n",
        "        challenger = mlflow.pyfunc.load_model(f\"models:/{self.model_name}@challenger\")\n",
        "        \n",
        "        # Track metrics\n",
        "        champion_metrics = {'predictions': [], 'latencies': [], 'errors': 0}\n",
        "        challenger_metrics = {'predictions': [], 'latencies': [], 'errors': 0}\n",
        "        \n",
        "        # Simulate requests\n",
        "        for i in range(n_requests):\n",
        "            # Sample random data point\n",
        "            idx = np.random.randint(0, len(X_test))\n",
        "            sample = pd.DataFrame([X_test[idx]], columns=wine.feature_names)\n",
        "            true_label = y_test[idx]\n",
        "            \n",
        "            # Route traffic\n",
        "            model_choice = self.route_traffic(challenger_traffic)\n",
        "            \n",
        "            # Make prediction and measure latency\n",
        "            import time\n",
        "            try:\n",
        "                if model_choice == 'challenger':\n",
        "                    start = time.time()\n",
        "                    pred = challenger.predict(sample)[0]\n",
        "                    latency = (time.time() - start) * 1000  # ms\n",
        "                    \n",
        "                    challenger_metrics['predictions'].append((pred, true_label))\n",
        "                    challenger_metrics['latencies'].append(latency)\n",
        "                else:\n",
        "                    start = time.time()\n",
        "                    pred = champion.predict(sample)[0]\n",
        "                    latency = (time.time() - start) * 1000  # ms\n",
        "                    \n",
        "                    champion_metrics['predictions'].append((pred, true_label))\n",
        "                    champion_metrics['latencies'].append(latency)\n",
        "            except Exception as e:\n",
        "                if model_choice == 'challenger':\n",
        "                    challenger_metrics['errors'] += 1\n",
        "                else:\n",
        "                    champion_metrics['errors'] += 1\n",
        "        \n",
        "        # Calculate results\n",
        "        results = self._calculate_metrics(champion_metrics, challenger_metrics)\n",
        "        return results\n",
        "    \n",
        "    def _calculate_metrics(self, champion_metrics, challenger_metrics):\n",
        "        \"\"\"Calculate A/B test metrics\"\"\"\n",
        "        from sklearn.metrics import accuracy_score\n",
        "        \n",
        "        results = {}\n",
        "        \n",
        "        for model_name, metrics in [('Champion', champion_metrics), ('Challenger', challenger_metrics)]:\n",
        "            if metrics['predictions']:\n",
        "                preds, labels = zip(*metrics['predictions'])\n",
        "                accuracy = accuracy_score(labels, preds)\n",
        "                avg_latency = np.mean(metrics['latencies'])\n",
        "                p95_latency = np.percentile(metrics['latencies'], 95)\n",
        "                \n",
        "                results[model_name] = {\n",
        "                    'requests': len(metrics['predictions']),\n",
        "                    'accuracy': accuracy,\n",
        "                    'avg_latency_ms': avg_latency,\n",
        "                    'p95_latency_ms': p95_latency,\n",
        "                    'errors': metrics['errors'],\n",
        "                    'error_rate': metrics['errors'] / (len(metrics['predictions']) + metrics['errors'])\n",
        "                }\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def progressive_rollout(self, stages=[0.05, 0.20, 0.50, 1.0], requests_per_stage=500):\n",
        "        \"\"\"Simulate progressive rollout of challenger model\"\"\"\n",
        "        print(\"PROGRESSIVE ROLLOUT SIMULATION\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        all_results = []\n",
        "        \n",
        "        for stage, traffic_pct in enumerate(stages):\n",
        "            print(f\"\\nStage {stage + 1}: {traffic_pct*100:.0f}% traffic to challenger\")\n",
        "            \n",
        "            results = self.simulate_production_traffic(\n",
        "                n_requests=requests_per_stage,\n",
        "                challenger_traffic=traffic_pct\n",
        "            )\n",
        "            \n",
        "            # Display stage results\n",
        "            stage_df = pd.DataFrame(results).T\n",
        "            display(stage_df)\n",
        "            \n",
        "            # Check if challenger is performing well\n",
        "            if 'Challenger' in results and 'Champion' in results:\n",
        "                if results['Challenger']['accuracy'] < results['Champion']['accuracy'] * 0.95:\n",
        "                    print(\"‚ö†Ô∏è WARNING: Challenger performing poorly, consider rollback\")\n",
        "                else:\n",
        "                    print(\"‚úì Challenger performing well, safe to continue\")\n",
        "            \n",
        "            all_results.append(results)\n",
        "        \n",
        "        return all_results\n",
        "\n",
        "# Run A/B Testing Simulation\n",
        "ab_test = ABTestingFramework(FULL_MODEL_NAME)\n",
        "\n",
        "# Simulate initial canary deployment (5% traffic)\n",
        "print(\"CANARY DEPLOYMENT (5% traffic to challenger)\")\n",
        "print(\"-\" * 60)\n",
        "canary_results = ab_test.simulate_production_traffic(n_requests=1000, challenger_traffic=0.05)\n",
        "display(pd.DataFrame(canary_results).T)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate Progressive Rollout\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PROGRESSIVE ROLLOUT STRATEGY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define rollout stages\n",
        "rollout_stages = [0.05, 0.20, 0.50, 1.0]  # 5% -> 20% -> 50% -> 100%\n",
        "rollout_results = ab_test.progressive_rollout(stages=rollout_stages, requests_per_stage=200)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Promotion Workflow\n",
        "\n",
        "### üìà Promoting Challenger to Champion\n",
        "\n",
        "Once A/B testing confirms the challenger performs better, follow this promotion workflow:\n",
        "\n",
        "1. **Review Metrics**: Ensure all KPIs meet thresholds\n",
        "2. **Approval Process**: Get stakeholder sign-off\n",
        "3. **Update Aliases**: Swap champion and challenger aliases\n",
        "4. **Archive Previous**: Keep previous champion for rollback\n",
        "5. **Monitor**: Watch for issues post-promotion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Promotion Workflow Implementation\n",
        "class ModelPromotion:\n",
        "    \"\"\"Manage model promotion from challenger to champion\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name, client):\n",
        "        self.model_name = model_name\n",
        "        self.client = client\n",
        "        \n",
        "    def validate_promotion_criteria(self, min_accuracy=0.90, min_f1=0.90, max_latency_ms=100):\n",
        "        \"\"\"Validate if challenger meets promotion criteria\"\"\"\n",
        "        \n",
        "        print(\"PROMOTION CRITERIA VALIDATION\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        criteria_met = True\n",
        "        validation_results = {}\n",
        "        \n",
        "        # Get challenger version\n",
        "        model_info = self.client.get_registered_model(self.model_name)\n",
        "        challenger_version = model_info.aliases.get('challenger')\n",
        "        \n",
        "        if not challenger_version:\n",
        "            print(\"‚ùå No challenger model found\")\n",
        "            return False, {}\n",
        "        \n",
        "        # Get run metrics\n",
        "        version_info = self.client.get_model_version(self.model_name, challenger_version)\n",
        "        run = self.client.get_run(version_info.run_id)\n",
        "        \n",
        "        # Check accuracy\n",
        "        test_accuracy = run.data.metrics.get('test_accuracy', 0)\n",
        "        if test_accuracy >= min_accuracy:\n",
        "            validation_results['accuracy'] = f\"‚úì Accuracy: {test_accuracy:.4f} >= {min_accuracy}\"\n",
        "        else:\n",
        "            validation_results['accuracy'] = f\"‚úó Accuracy: {test_accuracy:.4f} < {min_accuracy}\"\n",
        "            criteria_met = False\n",
        "        \n",
        "        # Check F1 score\n",
        "        test_f1 = run.data.metrics.get('test_f1', 0)\n",
        "        if test_f1 >= min_f1:\n",
        "            validation_results['f1'] = f\"‚úì F1 Score: {test_f1:.4f} >= {min_f1}\"\n",
        "        else:\n",
        "            validation_results['f1'] = f\"‚úó F1 Score: {test_f1:.4f} < {min_f1}\"\n",
        "            criteria_met = False\n",
        "        \n",
        "        # Display results\n",
        "        for criterion, result in validation_results.items():\n",
        "            print(f\"  {result}\")\n",
        "        \n",
        "        print(f\"\\nValidation Result: {'PASSED ‚úì' if criteria_met else 'FAILED ‚úó'}\")\n",
        "        \n",
        "        return criteria_met, validation_results\n",
        "    \n",
        "    def promote_model(self, require_approval=True):\n",
        "        \"\"\"Promote challenger to champion with proper governance\"\"\"\n",
        "        \n",
        "        print(\"\\nMODEL PROMOTION PROCESS\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        # Step 1: Validate criteria\n",
        "        criteria_met, validation = self.validate_promotion_criteria()\n",
        "        \n",
        "        if not criteria_met:\n",
        "            print(\"\\n‚ùå Promotion blocked: Criteria not met\")\n",
        "            return False\n",
        "        \n",
        "        # Step 2: Approval process (simulated)\n",
        "        if require_approval:\n",
        "            print(\"\\nüìã Approval Process:\")\n",
        "            print(\"  - Technical Review: ‚úì\")\n",
        "            print(\"  - Business Review: ‚úì\")\n",
        "            print(\"  - Risk Assessment: ‚úì\")\n",
        "            approval = True  # In production, this would be a real approval workflow\n",
        "            \n",
        "            if not approval:\n",
        "                print(\"\\n‚ùå Promotion blocked: Approval not granted\")\n",
        "                return False\n",
        "        \n",
        "        # Step 3: Execute promotion\n",
        "        try:\n",
        "            model_info = self.client.get_registered_model(self.model_name)\n",
        "            current_champion = model_info.aliases.get('champion')\n",
        "            current_challenger = model_info.aliases.get('challenger')\n",
        "            \n",
        "            if not current_challenger:\n",
        "                print(\"‚ùå No challenger version to promote\")\n",
        "                return False\n",
        "            \n",
        "            print(f\"\\nüîÑ Executing Promotion:\")\n",
        "            print(f\"  Current Champion: Version {current_champion}\")\n",
        "            print(f\"  Current Challenger: Version {current_challenger}\")\n",
        "            \n",
        "            # Archive current champion\n",
        "            if current_champion:\n",
        "                self.client.set_registered_model_alias(\n",
        "                    name=self.model_name,\n",
        "                    alias=\"previous_champion\",\n",
        "                    version=current_champion\n",
        "                )\n",
        "                print(f\"  ‚úì Archived Version {current_champion} as 'previous_champion'\")\n",
        "            \n",
        "            # Promote challenger to champion\n",
        "            self.client.set_registered_model_alias(\n",
        "                name=self.model_name,\n",
        "                alias=\"champion\",\n",
        "                version=current_challenger\n",
        "            )\n",
        "            print(f\"  ‚úì Promoted Version {current_challenger} to 'champion'\")\n",
        "            \n",
        "            # Remove challenger alias\n",
        "            self.client.delete_registered_model_alias(\n",
        "                name=self.model_name,\n",
        "                alias=\"challenger\"\n",
        "            )\n",
        "            print(f\"  ‚úì Removed 'challenger' alias\")\n",
        "            \n",
        "            # Update version description\n",
        "            promotion_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            self.client.update_model_version(\n",
        "                name=self.model_name,\n",
        "                version=current_challenger,\n",
        "                description=f\"Promoted to champion on {promotion_timestamp}. Previous champion: v{current_champion}\"\n",
        "            )\n",
        "            \n",
        "            # Log promotion event\n",
        "            print(f\"\\n‚úÖ PROMOTION SUCCESSFUL\")\n",
        "            print(f\"  New Champion: Version {current_challenger}\")\n",
        "            print(f\"  Promoted at: {promotion_timestamp}\")\n",
        "            \n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Promotion failed: {str(e)}\")\n",
        "            return False\n",
        "    \n",
        "    def rollback_model(self):\n",
        "        \"\"\"Rollback to previous champion if issues detected\"\"\"\n",
        "        \n",
        "        print(\"\\nMODEL ROLLBACK PROCESS\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        try:\n",
        "            model_info = self.client.get_registered_model(self.model_name)\n",
        "            current_champion = model_info.aliases.get('champion')\n",
        "            previous_champion = model_info.aliases.get('previous_champion')\n",
        "            \n",
        "            if not previous_champion:\n",
        "                print(\"‚ùå No previous champion to rollback to\")\n",
        "                return False\n",
        "            \n",
        "            print(f\"‚ö†Ô∏è Rolling back from Version {current_champion} to Version {previous_champion}\")\n",
        "            \n",
        "            # Set previous champion as current champion\n",
        "            self.client.set_registered_model_alias(\n",
        "                name=self.model_name,\n",
        "                alias=\"champion\",\n",
        "                version=previous_champion\n",
        "            )\n",
        "            \n",
        "            # Archive the failed version\n",
        "            self.client.set_registered_model_alias(\n",
        "                name=self.model_name,\n",
        "                alias=\"rolled_back\",\n",
        "                version=current_champion\n",
        "            )\n",
        "            \n",
        "            print(f\"‚úÖ Rollback successful\")\n",
        "            print(f\"  Current Champion: Version {previous_champion}\")\n",
        "            print(f\"  Rolled back version: {current_champion}\")\n",
        "            \n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Rollback failed: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "# Example: Promote the challenger model\n",
        "promoter = ModelPromotion(FULL_MODEL_NAME, client)\n",
        "\n",
        "# Validate promotion criteria\n",
        "can_promote, validation = promoter.validate_promotion_criteria(\n",
        "    min_accuracy=0.85,  # Lower threshold for demo\n",
        "    min_f1=0.85\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTIONAL: Execute promotion if criteria are met\n",
        "# Uncomment the following line to actually promote the model\n",
        "# if can_promote:\n",
        "#     promotion_success = promoter.promote_model(require_approval=True)\n",
        "\n",
        "print(\"\\nüí° To promote the model, uncomment and run the promotion code above\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Monitoring & Retraining Strategy\n",
        "\n",
        "### üìä Key Monitoring Metrics\n",
        "\n",
        "**Performance Monitoring:**\n",
        "- Model accuracy over time\n",
        "- Prediction latency\n",
        "- Error rates\n",
        "- Feature importance changes\n",
        "\n",
        "**Data Quality Monitoring:**\n",
        "- Input data distribution (drift detection)\n",
        "- Missing values\n",
        "- Outliers\n",
        "- Schema changes\n",
        "\n",
        "**Business Metrics:**\n",
        "- Business KPIs affected by the model\n",
        "- User feedback/satisfaction\n",
        "- Cost per prediction\n",
        "\n",
        "### üîÑ Retraining Triggers\n",
        "\n",
        "1. **Scheduled Retraining**: Weekly/Monthly/Quarterly\n",
        "2. **Performance-Based**: When metrics drop below threshold\n",
        "3. **Data-Driven**: When drift exceeds limits\n",
        "4. **Event-Based**: New data source, business changes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Monitoring Implementation\n",
        "class ModelMonitor:\n",
        "    \"\"\"Monitor model performance and trigger retraining when needed\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "        self.metrics_history = []\n",
        "        \n",
        "    def calculate_data_drift(self, reference_data, current_data):\n",
        "        \"\"\"Calculate drift between reference and current data distributions\"\"\"\n",
        "        from scipy import stats\n",
        "        \n",
        "        drift_scores = {}\n",
        "        \n",
        "        for col_idx, col_name in enumerate(wine.feature_names):\n",
        "            ref_col = reference_data[:, col_idx]\n",
        "            curr_col = current_data[:, col_idx]\n",
        "            \n",
        "            # Kolmogorov-Smirnov test for distribution difference\n",
        "            ks_statistic, p_value = stats.ks_2samp(ref_col, curr_col)\n",
        "            \n",
        "            drift_scores[col_name] = {\n",
        "                'ks_statistic': ks_statistic,\n",
        "                'p_value': p_value,\n",
        "                'drift_detected': p_value < 0.05  # 5% significance level\n",
        "            }\n",
        "        \n",
        "        # Overall drift score (average KS statistic)\n",
        "        overall_drift = np.mean([score['ks_statistic'] for score in drift_scores.values()])\n",
        "        \n",
        "        return overall_drift, drift_scores\n",
        "    \n",
        "    def monitor_performance(self, current_accuracy, current_f1, baseline_accuracy=0.90, baseline_f1=0.90):\n",
        "        \"\"\"Monitor model performance against baselines\"\"\"\n",
        "        \n",
        "        performance_status = {\n",
        "            'timestamp': datetime.now(),\n",
        "            'current_accuracy': current_accuracy,\n",
        "            'current_f1': current_f1,\n",
        "            'baseline_accuracy': baseline_accuracy,\n",
        "            'baseline_f1': baseline_f1,\n",
        "            'accuracy_degradation': baseline_accuracy - current_accuracy,\n",
        "            'f1_degradation': baseline_f1 - current_f1,\n",
        "            'requires_retraining': False\n",
        "        }\n",
        "        \n",
        "        # Check if retraining is needed\n",
        "        if current_accuracy < baseline_accuracy * 0.95:  # 5% degradation threshold\n",
        "            performance_status['requires_retraining'] = True\n",
        "            performance_status['reason'] = 'Accuracy degradation > 5%'\n",
        "        elif current_f1 < baseline_f1 * 0.95:\n",
        "            performance_status['requires_retraining'] = True\n",
        "            performance_status['reason'] = 'F1 score degradation > 5%'\n",
        "        \n",
        "        self.metrics_history.append(performance_status)\n",
        "        return performance_status\n",
        "    \n",
        "    def generate_monitoring_report(self, test_data, test_labels, reference_data):\n",
        "        \"\"\"Generate comprehensive monitoring report\"\"\"\n",
        "        \n",
        "        print(\"MODEL MONITORING REPORT\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Model: {self.model_name}\")\n",
        "        print(f\"Report Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(\"-\"*60)\n",
        "        \n",
        "        # Load current champion model\n",
        "        champion_model = mlflow.pyfunc.load_model(f\"models:/{self.model_name}@champion\")\n",
        "        \n",
        "        # Get predictions\n",
        "        predictions = champion_model.predict(pd.DataFrame(test_data, columns=wine.feature_names))\n",
        "        \n",
        "        # Calculate performance metrics\n",
        "        from sklearn.metrics import accuracy_score, f1_score\n",
        "        current_accuracy = accuracy_score(test_labels, predictions)\n",
        "        current_f1 = f1_score(test_labels, predictions, average='weighted')\n",
        "        \n",
        "        print(\"\\nüìä Performance Metrics:\")\n",
        "        print(f\"  Current Accuracy: {current_accuracy:.4f}\")\n",
        "        print(f\"  Current F1 Score: {current_f1:.4f}\")\n",
        "        \n",
        "        # Check data drift\n",
        "        overall_drift, feature_drift = self.calculate_data_drift(reference_data, test_data)\n",
        "        \n",
        "        print(f\"\\nüìà Data Drift Analysis:\")\n",
        "        print(f\"  Overall Drift Score: {overall_drift:.4f}\")\n",
        "        \n",
        "        # Count features with significant drift\n",
        "        drifted_features = [f for f, scores in feature_drift.items() if scores['drift_detected']]\n",
        "        print(f\"  Features with Drift: {len(drifted_features)}/{len(feature_drift)}\")\n",
        "        \n",
        "        if drifted_features:\n",
        "            print(\"  Drifted Features:\")\n",
        "            for feature in drifted_features[:5]:  # Show top 5\n",
        "                print(f\"    - {feature}: KS={feature_drift[feature]['ks_statistic']:.3f}\")\n",
        "        \n",
        "        # Performance monitoring\n",
        "        perf_status = self.monitor_performance(current_accuracy, current_f1)\n",
        "        \n",
        "        print(f\"\\n‚ö†Ô∏è Retraining Assessment:\")\n",
        "        if perf_status['requires_retraining']:\n",
        "            print(f\"  Status: RETRAINING RECOMMENDED\")\n",
        "            print(f\"  Reason: {perf_status['reason']}\")\n",
        "        else:\n",
        "            print(f\"  Status: Model performing within acceptable limits\")\n",
        "        \n",
        "        if overall_drift > 0.2:  # Drift threshold\n",
        "            print(f\"  ‚ö†Ô∏è High data drift detected (score: {overall_drift:.3f})\")\n",
        "            print(f\"     Consider retraining with recent data\")\n",
        "        \n",
        "        return {\n",
        "            'performance': perf_status,\n",
        "            'drift': {'overall': overall_drift, 'features': feature_drift},\n",
        "            'recommendations': self._get_recommendations(perf_status, overall_drift)\n",
        "        }\n",
        "    \n",
        "    def _get_recommendations(self, performance_status, drift_score):\n",
        "        \"\"\"Generate actionable recommendations\"\"\"\n",
        "        recommendations = []\n",
        "        \n",
        "        if performance_status['requires_retraining']:\n",
        "            recommendations.append(\"üî¥ Immediate: Retrain model with recent data\")\n",
        "        \n",
        "        if drift_score > 0.2:\n",
        "            recommendations.append(\"üü° Soon: Investigate data drift causes\")\n",
        "            recommendations.append(\"üü° Soon: Update training data distribution\")\n",
        "        \n",
        "        if drift_score > 0.1:\n",
        "            recommendations.append(\"üü¢ Monitor: Increase monitoring frequency\")\n",
        "        \n",
        "        if not recommendations:\n",
        "            recommendations.append(\"‚úÖ No action needed - model healthy\")\n",
        "        \n",
        "        return recommendations\n",
        "\n",
        "# Run monitoring simulation\n",
        "monitor = ModelMonitor(FULL_MODEL_NAME)\n",
        "\n",
        "# Simulate monitoring with current data\n",
        "monitoring_report = monitor.generate_monitoring_report(\n",
        "    test_data=X_test,\n",
        "    test_labels=y_test,\n",
        "    reference_data=X_train\n",
        ")\n",
        "\n",
        "print(\"\\nüìã Recommendations:\")\n",
        "for rec in monitoring_report['recommendations']:\n",
        "    print(f\"  {rec}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
