{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CSV Autoloader Streaming - Read from Volume and Write to Delta Table\n",
        "\n",
        "This notebook demonstrates **streaming ingestion** using Auto Loader (cloudFiles) to read CSV files from a Unity Catalog Volume and write to a Delta table.\n",
        "\n",
        "## Key Features:\n",
        "- **Streaming read** with Auto Loader for incremental processing\n",
        "- **Schema evolution** with automatic schema detection and storage\n",
        "- **Checkpoint management** for fault-tolerance and exactly-once processing\n",
        "- **Data transformations** with calculated columns (age + id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration Variables\n",
        "\n",
        "Define all paths and table names as variables for easy customization:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Catalog, schema, and table configuration\n",
        "CATALOG = \"jpg\"\n",
        "SCHEMA = \"default\"\n",
        "TABLE_NAME = \"csv_notebook\"\n",
        "FULL_TABLE_NAME = f\"{CATALOG}.{SCHEMA}.{TABLE_NAME}\"\n",
        "\n",
        "# Volume paths configuration\n",
        "VOLUME_BASE = f\"/Volumes/{CATALOG}/{SCHEMA}\"\n",
        "SOURCE_PATH = f\"{VOLUME_BASE}/csvs\"\n",
        "SCHEMA_LOCATION = f\"{VOLUME_BASE}/schemas/csv_autoloader_schema\"\n",
        "CHECKPOINT_LOCATION = f\"{VOLUME_BASE}/checkpoints/csv_autoloader_checkpoint\"\n",
        "\n",
        "# CSV reading options\n",
        "CSV_HEADER = \"true\"\n",
        "CSV_SEPARATOR = \",\"\n",
        "\n",
        "print(f\"Source Path: {SOURCE_PATH}\")\n",
        "print(f\"Target Table: {FULL_TABLE_NAME}\")\n",
        "print(f\"Schema Location: {SCHEMA_LOCATION}\")\n",
        "print(f\"Checkpoint Location: {CHECKPOINT_LOCATION}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read CSV Files Using Auto Loader (Streaming)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read CSV files from volume using Auto Loader with schema location\n",
        "df_raw = spark.readStream \\\n",
        "  .format(\"cloudFiles\") \\\n",
        "  .option(\"cloudFiles.format\", \"csv\") \\\n",
        "  .option(\"cloudFiles.schemaLocation\", SCHEMA_LOCATION) \\\n",
        "  .option(\"header\", CSV_HEADER) \\\n",
        "  .option(\"sep\", CSV_SEPARATOR) \\\n",
        "  .load(SOURCE_PATH)\n",
        "\n",
        "display(df_raw)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, current_timestamp\n",
        "\n",
        "# Apply transformations\n",
        "df_transformed = df_raw \\\n",
        "  .withColumn(\"age_plus_id\", col(\"age\").cast(\"int\") + col(\"id\").cast(\"int\")) \\\n",
        "  .withColumn(\"processed_timestamp\", current_timestamp())\n",
        "\n",
        "display(df_transformed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write Transformed Data to Delta Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write transformed data to Delta table with availableNow trigger (recommended for serverless)\n",
        "query = df_transformed.writeStream \\\n",
        "  .format(\"delta\") \\\n",
        "  .outputMode(\"append\") \\\n",
        "  .trigger(availableNow=True) \\\n",
        "  .option(\"checkpointLocation\", CHECKPOINT_LOCATION) \\\n",
        "  .toTable(FULL_TABLE_NAME)\n",
        "\n",
        "query.awaitTermination()\n",
        "print(f\"Successfully processed all available data to {FULL_TABLE_NAME}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
