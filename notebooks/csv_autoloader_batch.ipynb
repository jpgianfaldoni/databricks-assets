{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CSV Batch Reader - Read from Volume and Write to Delta Table\n",
        "\n",
        "This notebook demonstrates **batch ingestion** to read CSV files from a Unity Catalog Volume and write to a Delta table.\n",
        "\n",
        "## Key Features:\n",
        "- **Batch read** using standard Spark CSV reader for one-time or scheduled processing\n",
        "- **Schema inference** with automatic schema detection\n",
        "- **Efficient processing** of all files in a single batch operation\n",
        "\n",
        "## Difference from Streaming:\n",
        "- **Batch**: Reads all data at once, completes, and stops (this notebook)\n",
        "- **Streaming**: Continuously monitors for new data and processes incrementally (use Auto Loader with `readStream`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration Variables\n",
        "\n",
        "Define all paths and table names as variables for easy customization:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Catalog, schema, and table configuration\n",
        "CATALOG = \"jpg\"\n",
        "SCHEMA = \"default\"\n",
        "TABLE_NAME = \"csv_batch\"\n",
        "FULL_TABLE_NAME = f\"{CATALOG}.{SCHEMA}.{TABLE_NAME}\"\n",
        "\n",
        "# Volume paths configuration\n",
        "VOLUME_BASE = f\"/Volumes/{CATALOG}/{SCHEMA}\"\n",
        "SOURCE_PATH = f\"{VOLUME_BASE}/csvs\"\n",
        "\n",
        "\n",
        "print(f\"Source Path: {SOURCE_PATH}\")\n",
        "print(f\"Target Table: {FULL_TABLE_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read CSV Files Using Spark Batch Reader\n",
        "\n",
        "For batch processing, use the standard Spark CSV reader. This reads all files at once and is ideal for one-time loads or scheduled batch jobs.\n",
        "\n",
        "**Note**: Auto Loader (`cloudFiles`) is designed for streaming with `readStream`, not batch `read` operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read CSV files from volume using standard Spark batch reader\n",
        "df = spark.read \\\n",
        "  .format(\"csv\") \\\n",
        "  .option(\"header\", \"true\") \\\n",
        "  .option(\"sep\", \",\") \\\n",
        "  .option(\"inferSchema\", \"true\") \\\n",
        "  .load(SOURCE_PATH)\n",
        "\n",
        "# Display the data\n",
        "display(df)\n",
        "\n",
        "# Show basic statistics\n",
        "print(f\"Total rows: {df.count()}\")\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write to Delta Table (Batch)\n",
        "\n",
        "Write all the data to a Delta table in a single batch operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write to Delta table in batch mode\n",
        "df.write \\\n",
        "  .format(\"delta\") \\\n",
        "  .mode(\"overwrite\") \\\n",
        "  .saveAsTable(FULL_TABLE_NAME)\n",
        "\n",
        "print(f\"Successfully wrote all data to {FULL_TABLE_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative Write Modes\n",
        "\n",
        "The code above uses `mode(\"overwrite\")` which replaces the entire table. Other options include:\n",
        "\n",
        "- **`append`**: Add new rows to existing table\n",
        "- **`overwrite`**: Replace entire table (used above)\n",
        "- **`ignore`**: Write only if table doesn't exist\n",
        "- **`error` or `errorifexists`**: Throw error if table exists (default)\n",
        "\n",
        "### Example: Append Mode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative: Append mode (uncomment to use)\n",
        "# This will add new rows to the existing table without deleting old data\n",
        "\n",
        "# df.write \\\n",
        "#   .format(\"delta\") \\\n",
        "#   .mode(\"append\") \\\n",
        "#   .saveAsTable(FULL_TABLE_NAME)\n",
        "# \n",
        "# print(f\"Successfully appended data to {FULL_TABLE_NAME}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
