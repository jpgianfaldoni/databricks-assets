{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read CSV files from volume using autoloader with schema location\n",
        "df = spark.readStream \\\n",
        "  .format(\"cloudFiles\") \\\n",
        "  .option(\"cloudFiles.format\", \"csv\") \\\n",
        "  .option(\"cloudFiles.schemaLocation\", \"/Volumes/jpg/default/schemas/csv_autoloader_schema\") \\\n",
        "  .option(\"header\", \"true\") \\\n",
        "  .option(\"sep\", \",\") \\\n",
        "  .load(\"/Volumes/jpg/default/csvs\")\n",
        "\n",
        "# For serverless compute - write to Delta table with availableNow trigger\n",
        "query = df.writeStream \\\n",
        "  .format(\"delta\") \\\n",
        "  .outputMode(\"append\") \\\n",
        "  .trigger(availableNow=True) \\\n",
        "  .option(\"checkpointLocation\", \"/Volumes/jpg/default/checkpoints/csv_autoloader_checkpoint\") \\\n",
        "  .toTable(\"jpg.default.csv_notebook\")\n",
        "\n",
        "query.awaitTermination()\n",
        "\n",
        "# Alternative: Use once=True trigger (uncomment if preferred)\n",
        "# query = df.writeStream \\\n",
        "#   .format(\"delta\") \\\n",
        "#   .outputMode(\"append\") \\\n",
        "#   .trigger(once=True) \\\n",
        "#   .option(\"checkpointLocation\", \"/Volumes/jpg/default/checkpoints/csv_autoloader_checkpoint\") \\\n",
        "#   .toTable(\"jpg.default.csv_notebook\")\n",
        "# \n",
        "# query.awaitTermination()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
